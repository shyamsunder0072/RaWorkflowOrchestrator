import os
import pprint
import shutil
from pathlib import Path
from datetime import datetime, timedelta

from airflow import DAG
from airflow.settings import EDA_HOME, DAGS_FOLDER
from airflow.operators.dag_operator import DagOperator
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.api.common.experimental.delete_dag import delete_dag

pp = pprint.PrettyPrinter(indent=4)

classPath = 'ai.couture.obelisk.eda.MainClass'
appName = 'EDAVisualisations'
code_artifact = 'obelisk-eda.jar'

folder_to_copy_summary = '{{folder_to_copy_sum}}'

# path in container where intermediate data is dumped
data_dump_dir = Path(EDA_HOME).joinpath(*["inputs", folder_to_copy_summary, "preliminary"])

# folders inside data_dump_dir containing table_info and preliminary_summary
table_score_dump_dir = data_dump_dir.joinpath("table_score")
imp_features_dump_dir = data_dump_dir.joinpath("important_features")

# file where visualizations will be dumped.
viz_dump_path = Path(EDA_HOME).joinpath(
    *["outputs",
    {% for output_dir in output_dirs %}
    "{{output_dir}}",
    {% endfor %}
    "table_and_feature_importance.html"])


def mkdir_dirs(ds, **kwargs):
    # create paths to input output if not exists
    data_dump_dir.mkdir(parents=True, exist_ok=True)
    viz_dump_path.parent.mkdir(parents=True, exist_ok=True)


# how to cleanup when run failed ?
def cleanup_dirs(ds, **kwargs):
    shutil.rmtree(data_dump_dir.parent)


def remove_dags(ds, **kwargs):
    # remove the dags files, then remove from database.
    os.remove(os.path.join(DAGS_FOLDER, '{{dag_id}}.py'))
    os.remove(os.path.join(DAGS_FOLDER, '{{eda_dag_id}}.py'))
    delete_dag('{{eda_dag_id}}')
    delete_dag('{{dag_id}}')


default_args = {
    'owner': '{{username}}',
    'depends_on_past': False,
    'start_date': datetime({{now.year}}, {{now.month}}, {{now.day}}),
    'retries': 0,
}

EDADag = DAG(
    '{{dag_id}}',
    default_args=default_args,
    schedule_interval='@once')

EDADataScoreAndFeatureCreateDirs = PythonOperator(
    task_id='EDADataScoreAndFeatureCreateLocalDumpDirs',
    provide_context=True,
    python_callable=mkdir_dirs,
    dag=EDADag
)

EDADataScoreAndFeatureRemoveDirs = PythonOperator(
    task_id='EDADataScoreAndFeatureRemoveLocalDumpDirs',
    provide_context=True,
    python_callable=cleanup_dirs,
    dag=EDADag
)

EDADataScoreAndFeatureImportance = DagOperator(
    task_id='EDADataScoreAndFeatureImportance',
    run_dag_id='{{eda_dag_id}}',
    dag=EDADag,
    description=''
)

EDADataScoreAndFeatureVisualisation = BashOperator(
    task_id='EDADataScoreAndFeatureVisualisation',
    dag=EDADag,
    description='EDADataScoreAndFeatureVisualisation',
    bash_command='python3 {} {} {} {}'.format(
        Path(EDA_HOME).joinpath(*['codes', 'table_score_important_features_visualizations.py']),
        table_score_dump_dir,
        imp_features_dump_dir,
        viz_dump_path)
)

EDADataScoreAndFeatureRemoveDAGs = PythonOperator(
    task_id='EDADataScoreAndFeatureRemoveDAGs',
    provide_context=True,
    python_callable=remove_dags,
    dag=EDADag
)

# hdfs dfs -get source/path dest/path
EDADataScoreAndFeatureImportance.set_upstream(EDADataScoreAndFeatureCreateDirs)
EDADataScoreAndFeatureVisualisation.set_upstream(EDADataScoreAndFeatureImportance)
EDADataScoreAndFeatureRemoveDirs.set_upstream(EDADataScoreAndFeatureVisualisation)
EDADataScoreAndFeatureRemoveDAGs.set_upstream(EDADataScoreAndFeatureRemoveDirs)
