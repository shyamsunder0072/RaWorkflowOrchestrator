import os
import shutil

from datetime import datetime
from pathlib import Path
from airflow import DAG
from airflow.operators import CoutureSparkOperator, HDFSOperator
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import BranchPythonOperator, PythonOperator
from airflow.settings import EDA_HOME


classPath = 'ai.couture.obelisk.eda.MainClass'
appName = 'EDAUniVariateBriefDataSummary'
code_artifact = 'obelisk-eda.jar'


# HDFS testing URI
uri = '{{source.connection_uri}}'
file_name = uri.split("/")[-1]
file_folder = file_name.split(".")[0]

folder_to_copy_summary = '{{folder_to_copy_sum}}'

{% if source.source_type == 'local'  %}
rawDirPath = '/data/eda/raw/inputfiles/' + folder_to_copy_summary
if not rawDirPath.endswith('/'):
    rawDirPath += '/'
{% else %}
uripath = uri.split("://")[1].replace(file_name, "")
rawDirPath = uripath
{% endif %}

def get_source():
    if "hdfs://" in uri:
        source = "CreateOutputHDFSPathIfNotExists"
    elif '{{source.source_type}}' == 'local':
        source = 'MakeEdaSourceHDFSDataDir'
    else:
        source = "GenerateSummaryDB"
    return source


source = get_source()


def get_hdfs_processed_dir_path():
    pth = str(Path('/data/eda/processed/outputfiles/').joinpath(*[folder_to_copy_summary, "preliminary"]))

    if not pth.endswith('/'):
        pth += '/'
    return pth


HdfsProcessedDirPath = get_hdfs_processed_dir_path()

# local file path is the file path on the webserver, where we want to output
# keep preliminary data

# This path is the same as data_dump_dir in EDA_preliminary_visualisations
data_dump_dir = Path(EDA_HOME).joinpath(*['inputs', folder_to_copy_summary, 'preliminary'])

data_dump_dir_hdfs = Path(EDA_HOME).joinpath(*["inputs", folder_to_copy_summary])


# def mkdir_dirs(ds, **kwargs):
#     if source == "GenerateSummaryDB":
#         data_dump_dir.mkdir(parents=True, exist_ok=True)
#     else:
#         data_dump_dir_hdfs.mkdir(parents=True, exist_ok=True)


default_args = {
    'owner': '{{username}}',
    'depends_on_past': False,
    'start_date': datetime({{now.year}}, {{now.month}}, {{now.day}}),
    'retries': 0,
}

EDADag = DAG(
    '{{dag_id}}',
    default_args=default_args,
    schedule_interval=None)

# EDAPreliminaryCreateDirs = PythonOperator(
#     task_id='CreateDirs',
#     provide_context=True,
#     python_callable=mkdir_dirs,
#     dag=EDADag
# )

branch_task = BranchPythonOperator(
    task_id='branching',
    python_callable=get_source,
    dag=EDADag,
)

{% if source.source_type == 'local'  %}
put_path = rawDirPath
MakeEdaSourceHDFSDataDir = HDFSOperator(
    task_id='MakeEdaSourceHDFSDataDir',
    dag=EDADag,
    description='',
    command_arguments=['-mkdir', '-p', put_path]
)
CopyEdaSourceToHDFS = HDFSOperator(
    task_id='CopyEdaSourceToHDFS',
    dag=EDADag,
    description='',
    command_arguments=['-put', '-f', '{{source.connection_uri}}', put_path]
)
{% endif %}

CreateOutputHDFSPathIfNotExists = HDFSOperator(
    task_id='CreateOutputHDFSPathIfNotExists',
    dag=EDADag,
    description='',
    command_arguments=['-mkdir', '-p', str(Path(HdfsProcessedDirPath).parent)]
)

GeneratePreliminarySummaryHDFS = CoutureSparkOperator(
    task_id='GeneratePreliminarySummaryHDFS',
    app_name=appName,
    class_path=classPath,
    code_artifact=code_artifact,
    input_base_dir_path=str(rawDirPath),
    output_base_dir_path=str(HdfsProcessedDirPath),
    input_filenames_dict={"data": str(file_name)},
    output_filenames_dict={"preliminary_data_summary": "preliminary_summary"},
    dag=EDADag,
    description='')


# only used with "GeneratePreliminarySummaryDB"
op_kwargs = {
    'uri': '{{source.connection_uri}}',
    'local_output_path': data_dump_dir,
    'table_name': '{{source.tablename}}'
}

GeneratePreliminarySummaryDB = BashOperator(
    task_id='GenerateSummaryDB',
    depends_on_past=False,
    bash_command='python3 "{}" "{}" "{}" "{}"'.format(
        os.path.join(EDA_HOME, *["codes", "preliminary_summary_db.py"]),
        op_kwargs['uri'], op_kwargs['table_name'],
        op_kwargs['local_output_path']),
    dag=EDADag
)

GetTableInfo = CoutureSparkOperator(
    task_id='GetTableInformation',
    app_name=appName,
    class_path=classPath,
    code_artifact=code_artifact,
    input_base_dir_path=str(rawDirPath),
    output_base_dir_path=str(HdfsProcessedDirPath),
    input_filenames_dict={"data": str(file_name)},
    output_filenames_dict={"data_info": "table_info"},
    dag=EDADag,
    description='')

CopyFilesToLocal = HDFSOperator(
    task_id='CopySummaryData',
    dag=EDADag,
    description='',
    command_arguments=['-get', str(Path(HdfsProcessedDirPath).parent), str(data_dump_dir_hdfs)]
)

def conditionally_trigger(context, dag_run_obj):
    c_p = context['params']['condition_param']
    print("Controller DAG : conditionally_trigger = {}".format(c_p))
    if context['params']['condition_param']:
        dag_run_obj.payload = {'message': context['params']['message']}
        # pp.pprint(dag_run_obj.payload)
        return dag_run_obj


# branch_task.set_upstream(EDAPreliminaryCreateDirs)
{% if source.source_type == 'local' %}
MakeEdaSourceHDFSDataDir.set_upstream(branch_task)
CopyEdaSourceToHDFS.set_upstream(MakeEdaSourceHDFSDataDir)
CreateOutputHDFSPathIfNotExists.set_upstream(CopyEdaSourceToHDFS)
{% else %}
CreateOutputHDFSPathIfNotExists.set_upstream(branch_task)
{% endif %}
GeneratePreliminarySummaryHDFS.set_upstream(CreateOutputHDFSPathIfNotExists)
GetTableInfo.set_upstream(GeneratePreliminarySummaryHDFS)
CopyFilesToLocal.set_upstream(GetTableInfo)

GeneratePreliminarySummaryDB.set_upstream(branch_task)
