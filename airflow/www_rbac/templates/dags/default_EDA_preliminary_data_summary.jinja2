import os
import shutil

from datetime import datetime
from pathlib import Path
from airflow import DAG
from airflow.operators import CoutureSparkOperator
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import BranchPythonOperator, PythonOperator
from airflow.settings import EDA_HOME


classPath = 'ai.couture.obelisk.eda.MainClass'
appName = 'EDAUniVariateBriefDataSummary'
code_artifact = 'obelisk-eda.jar'

# Database testing URI
# uripath = "mysql://couture:couture@mysql/couture"
# file_name = "dag_run"

# HDFS testing URI
# uri = 'hdfs://data/ecomm/ajio/processed/productAttributes'
uri = '{{source.connection_uri}}'
file_name = uri.split("/")[-1]
file_folder = file_name.split(".")[0]
uripath = uri.split(":/")[1].replace(file_name, "")

folder_to_copy_summary = '{{folder_to_copy_sum}}'


def get_source():
    if "hdfs" in uri:
        source = "GeneratePreliminarySummaryHDFS"
    else:
        source = "GeneratePreliminarySummaryDB"
    return source


source = get_source()


rawDirPath = uripath

# dfs create dir.
HdfsProcessedDirPath = str(Path('/data/eda/processed/outputfiles/').joinpath(*[folder_to_copy_summary, "preliminary"]))

if not HdfsProcessedDirPath.endswith('/'):
    HdfsProcessedDirPath += '/'


# local file path is the file path on the webserver, where we want to output
# keep preliminary data

# This path is the same as data_dump_dir in EDA_preliminary_visualisations
data_dump_dir = Path(EDA_HOME).joinpath(*['inputs', folder_to_copy_summary, 'preliminary'])

data_dump_dir_hdfs = Path(EDA_HOME).joinpath(*["inputs", folder_to_copy_summary])


def mkdir_dirs(ds, **kwargs):
    if source != "GeneratePreliminarySummaryHDFS":
        data_dump_dir.mkdir(parents=True, exist_ok=True)
    else:
        data_dump_dir_hdfs.mkdir(parents=True, exist_ok=True)


default_args = {
    'owner': '{{username}}',
    'depends_on_past': False,
    'start_date': datetime({{now.year}}, {{now.month}}, {{now.day}}),
    'retries': 0,
}

EDADag = DAG(
    '{{dag_id}}',
    default_args=default_args,
    schedule_interval=None)

EDAPreliminaryCreateDirs = PythonOperator(
    task_id='CreateDirs',
    provide_context=True,
    python_callable=mkdir_dirs,
    dag=EDADag
)

branch_task = BranchPythonOperator(
    task_id='branching',
    python_callable=get_source,
    dag=EDADag,
)

CreateOutputHDFSPathIfNotExists = BashOperator(
    task_id='CreateOutputHDFSPathIfNotExists',
    dag=EDADag,
    description='',
    bash_command='hdfs dfs -mkdir -p {}'.format(Path(HdfsProcessedDirPath).parent)
)

GeneratePreliminarySummaryHDFS = CoutureSparkOperator(
    task_id='GenerateSummaryHDFS',
    app_name=appName,
    class_path=classPath,
    code_artifact=code_artifact,
    input_base_dir_path=str(rawDirPath),
    output_base_dir_path=str(HdfsProcessedDirPath),
    input_filenames_dict={"data": str(file_name)},
    output_filenames_dict={"preliminary_data_summary": "preliminary_summary"},
    dag=EDADag,
    description='')


# only used with "GeneratePreliminarySummaryDB"
op_kwargs = {
    'uri': '{{source.connection_uri}}',
    'local_output_path': data_dump_dir,
    'table_name': '{{source.tablename}}'
}

GeneratePreliminarySummaryDB = BashOperator(
    task_id='GenerateSummaryDB',
    depends_on_past=False,
    bash_command='python3 "{}" "{}" "{}" "{}"'.format(
        os.path.join(EDA_HOME, *["codes", "preliminary_summary_db.py"]),
        op_kwargs['uri'], op_kwargs['table_name'],
        op_kwargs['local_output_path']),
    dag=EDADag
)

GetTableInfo = CoutureSparkOperator(
    task_id='GetTableInformation',
    app_name=appName,
    class_path=classPath,
    code_artifact=code_artifact,
    input_base_dir_path=str(rawDirPath),
    output_base_dir_path=str(HdfsProcessedDirPath),
    input_filenames_dict={"data": str(file_name)},
    output_filenames_dict={"data_info": "table_info"},
    dag=EDADag,
    description='')


def true_cmd(del_directory_cmd, copy_directory_cmd):
    if os.path.isdir(del_directory_cmd):
        return del_directory_cmd + ' && ' + copy_directory_cmd
    else:
        return copy_directory_cmd


del_directory_cmd = 'rm -r {}'.format(Path(data_dump_dir_hdfs))
copy_directory_cmd = 'hdfs dfs -get {} {}'.format(Path(HdfsProcessedDirPath), data_dump_dir_hdfs)
cmd = true_cmd(del_directory_cmd, copy_directory_cmd)
CopyFilesToLocal = BashOperator(
    task_id='CopySummaryData',
    dag=EDADag,
    description='',
    bash_command=cmd
)


def conditionally_trigger(context, dag_run_obj):
    c_p = context['params']['condition_param']
    print("Controller DAG : conditionally_trigger = {}".format(c_p))
    if context['params']['condition_param']:
        dag_run_obj.payload = {'message': context['params']['message']}
        # pp.pprint(dag_run_obj.payload)
        return dag_run_obj


branch_task.set_upstream(EDAPreliminaryCreateDirs)
CreateOutputHDFSPathIfNotExists.set_upstream(branch_task)
GeneratePreliminarySummaryHDFS.set_upstream(CreateOutputHDFSPathIfNotExists)
GeneratePreliminarySummaryDB.set_upstream(branch_task)
GetTableInfo.set_upstream(GeneratePreliminarySummaryHDFS)
CopyFilesToLocal.set_upstream(GetTableInfo)
